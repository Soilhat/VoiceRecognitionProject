---
title: "GenderRecognition"
output: html_document
editor_options: 
  chunk_output_type: console
author: ["Valentin Buisson", "Louis Gaillet", "Soilhat Mohamed", "SÃ©bastien Roques"]
date: "`r Sys.Date()`"
#knit: "bookdown::render_book"
site: bookdown::bookdown_site
documentclass: book
description: "Gender Recognition serves AI and Chatbots to the identification of the gender of their interlocutor"
github-repo: Soilhat/VoiceRecognitionProject
url: 'https://github.com/Soilhat/VoiceRecognitionProject'
---

# Initialisation Dataset

First we needed to find a large dataset to train our models : we inally found it on the internet.

Each voice sample is stored as a .WAV file, which is then pre-processed for acoustic analysis using the specan function from the WarbleR R package. Specan measures 22 acoustic parameters on acoustic signals for which the start and end times are provided.

The output from the pre-processed WAV files were saved into a CSV file, containing 3168 rows and 21 columns (20 columns for each feature and one label column for the classification of male or female). You can download the pre-processed dataset in CSV format, using the link above.

Let's see the first sample :

```{r}
dataset <- read.csv("voice.csv")
dataset[1,]
```
Here are the information we've got :

* duration: length of signal
* meanfreq: mean frequency (in kHz)
* sd: standard deviation of frequency
* median: median frequency (in kHz)
* Q25: first quantile (in kHz)
* Q75: third quantile (in kHz)
* IQR: interquantile range (in kHz)
* skew: skewness (see note in specprop description)
* kurt: kurtosis (see note in specprop description)
* sp.ent: spectral entropy
* sfm: spectral flatness
* mode: mode frequency
* centroid: frequency centroid (see specprop)
* peakf: peak frequency (frequency with highest energy)
* meanfun: average of fundamental frequency measured across acoustic signal
* minfun: minimum fundamental frequency measured across acoustic signal
* maxfun: maximum fundamental frequency measured across acoustic signal
* meandom: average of dominant frequency measured across acoustic signal
* mindom: minimum of dominant frequency measured across acoustic signal
* maxdom: maximum of dominant frequency measured across acoustic signal
* dfrange: range of dominant frequency measured across acoustic signal
* modindx: modulation index. Calculated as the accumulated absolute difference between adjacent measurements of fundamental frequencies divided by the frequency range


Now we need to split the dataset into 2 part : the training set and the test set.
```{r}
library(caTools);
set.seed('79')
split = sample.split(dataset$label, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
```

# Logistic Regression

In our case we have a huge amount of informations and variables in our dataset. We new to find witch variables are the more efficient in a gender recognition. Instinctively, we can say that the frequency is a great indicator in this case. Let's see if this is real by taking the average frequency measurement:

```{r}
summary(glm(formula = label ~ meandom, family = "binomial", data = training_set))
```

We can now be sure that the frequency is a great indicator. What about the others ?

```{r}
summary(glm(formula = label ~ ., family = "binomial", data = training_set))
```

We have here 8 significant properties that help define the gender.
We decided to do the logistic regression on all the variables provided.

```{r}
model <- glm(formula = label ~ ., family = "binomial", data = training_set)
```

Once the model is done, we have to predict the gender on the test set and do a convolution matrix to observe the result

```{r}
predictor = predict(model, newdata = test_set, type="response")
avgpredictor = ifelse(predictor > 0.5, 1,0)
cm = table(test_set[,21], avgpredictor)
mosaicplot(cm,col=sample(1:8,2))
```

We can see that the model is very efficient with a __97%__ accuracy. Let's try this test with other models.

# Random Forest

```{r}
# install.packages("randomForest")
library(randomForest)
```
```{r}
modelForest <- randomForest(label ~., data= training_set, ntree = 500, na.action = na.omit)
modelForest
```
Here we specify with the parameter *na.action = na.omit* to omit the unknown values. We find an error rate of 2% so a ___98 %___ accuracy on the training set, the result is similar as the model with Logistic Regression.

Now lets do a prediction:
```{r}
cmF <- table(predict(modelForest, test_set), test_set$label)
(cmF[1,1] + cmF[2,2]) / sum(cmF) # accuracy
```
We find a __98%__ accuracy on the test set. That's a higher accuracy than the Logistic Regression.

# XGBoost

```{r}
library(xgboost)
library(car)
y <- recode(training_set$label," 'female'=0; 'male'=1")
xgb <- xgboost(data = data.matrix(training_set), label = training_set$label, nrounds = 25);
pred <- predict(xgb,data.matrix(test_set))
table(pred, test_set$label)
```

Here We find a __100%___ accuracy on the test set. That's the best model so far.